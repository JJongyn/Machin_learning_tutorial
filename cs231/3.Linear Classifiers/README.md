# 3. Linear Classifiers

## 선형 분류기

3장에서는 선형 분류 기법에 대해서 알아보고 이 모델이 최적의 파라미터를 가지기 위한 정량적 기법 loss function에 대해서도 알아보고자 한다.

![12](./img/498_FA2019_lecture031024_12.jpg)
위의 그림을 보면 (32x32x3)의 이미지가 입력이 되고(x), 최적의 모델을 찾기위해 조정되는 파라미터(W, b)값이 존재한다. 이때 출력으로 10개의 class score가 나와야 하므로 W값의 크기는 (10, 3072), b는 (10,0)이 되면 f = Wx + b의 크기를 충족할 수 있다.

이제 선형 분류기를 3가지의 관점을 통해 확인해보자.

### 1. Algebraic Viewpoint

![15](./img/498_FA2019_lecture031024_15.jpg)
행렬의 형태로 모든 값들을 표현하면 위와 같다. W, b 값을 초기화해주고 입력값과 계산하여 최종 score를 출력한다. 이때 출력에 해당하는 (3,)차원의 행렬이 각 class에 대한 score가 된다. 위에서 부터 1, 2, 3번 class라고 한다면 2번 class에 대한 score가 가장 높은 것을 확인할 수 있다. 방법에 따라 bias를 W 행렬 벡터에 추가해주고 flatten된 입력 이미지에 1을 추가해서도 score를 계산할 수 있다.


### 2. Visual Viewpoint
![23](./img/498_FA2019_lecture031024_23.jpg)
위의 그림은 선형 분류기를 통해 학습된 각 class의 template이다. 여기서 의미하는 것은 우리가 어떤 이미지를 예측하고자 한다면 위의 template과 가장 유사한 class로 예측을 하게 되는 것이다. 하지만 하나의 template만을 사용하기 때문에 색, 방향 등에 따른 정확한 class를 예측하기 어렵다.

### 3. Geometric Viewpoint
![28](./img/498_FA2019_lecture031024_28.jpg)

## 선형 분류기의 한계
![30](./img/498_FA2019_lecture031024_30.jpg)


위의 초록색, 파란색 점을 분류할 수 있는 한 개의 직선이 없다. 즉, 선형 분류기는 XOR 문제를 해결하지 못하는 한계를 가진다.

지금까지 선형 분류기의 전체 흐름을 보았다. 여기서 생기는 궁금증은 그럼 파라미터 W, b 값은 어떻게 선택할 수 있을까이다.
또한 **선택된 파라미터가 현재 모델에 얼마나 적합한 모델인지, 판단할 수 있는 정량적인 값**이 필요하다. 이를 위해 Loss Function이라는 개념을 도입한다.

## Loss Function

![39](./img/498_FA2019_lecture031024_39.jpg)

단순한 Loss function을 살펴보면 현재 입력 데이터 x, 가중치 파라미터 W 값을 통해 나온 score, y_pred 값과 입력 데이터에 대한 정답값인 y값을 Loss function에 넣음으로써 loss를 계산할 수 있다. 

대표적인 Loss function의 예를 살펴보자.

### 1. SVM Loss

![43](./img/498_FA2019_lecture031024_43.jpg)
